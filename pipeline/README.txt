OVERVIEW
  The data_management_pipeline.py script mainly does 2 things:
    1.  Launch jobs for pre-defined pipelines.
    2.  Records parameters for all lauched jobs.

ACTIVATING THE ENVIRONMENT
  To run this pipeline, the data management python environment needs to be activated to ensure dependencies are present.  This is done with the following command on the IHG cluster:
    source /home/sequencing/.virtual_envs/data_management_env/bin/activate
  Since this environment uses python 2.5, the correct python library needs to be connected:
    export LD_LIBRARY_PATH=/home/sequencing/local/lib
  Finally, to ensure that python can find the local scripts, the python path needs to be set:
    export PYTHONPATH=/home/sequencing/src/pipeline_project/pipeline:$PYTHONPATH

LOCAL MODULES/SUB-DIRECTORIES
  Jobs are instantiated via objects which I will discuss more below, and the mockdb application stores these objects in csv files spelled out in the system configuarion file.  To facilitate these functions, a number of modules have been developed:

    class_pedigree:  This application details the inheritance of objects, and returns lists of either ancestor classes (classes from which an object has inheritted behavior) or children classes (classes for which the object is an ancestor).  These lists are used heavily in the mockdb application.

    config:  This heavily uses Python's native ConfigParser.  The system and pipeline specific configuration files are stored in the directory (for instance, system: config/ihg_system.cfg and pipeline: config/std_on_ihg.cfg) are all stored as files ending with the extension cfg.  config/scripts.py contains wrappers to handle specific config entries and return them in the way they are accessed in the pipeline.

    demultiplex_stats:  This application is used after running casava to extract statistics from this process and store it in relevant physical and process objects.

    manage_storage:  This application defines the StorageDevice object which keeps track of specific directories, available storage, and used storage in the manage_storage/models.py.  Queries to the disk (like disk available and disk usage) are handled by manage_storage/disk_queries.py, and more complicated accounting of disk usage for a specified pipeline are handled in managae_storage/scripts.py

    mockdb: This application stores objects as csv files for later use.  This is similar to packages like sql3lite, but since our sys-admin was against installing such packages, I wrote this.  This is at the heart of the data_mangagement.py script since it permanently stores the objects for continued use.  It reads objects created in the models.py file in directories that are supplied to mockdb from the system configuration file.

    sge_email:  This application hard-codes the parameters for sending emails on the IHG cluster in the sge_email/models.py file and calls the python packages needed to send an email in sge_email/scripts.py file.  This application may be called from the command line (python sge_email.py -h to obtain information on how to send an email).

    sge_queries: This application provides an interface with system level quereies (like qstat).

    physical_objects:  This collection of object definitions defines the attributes of truly physical objects like Sample, Flowcell, and HiSeq machine that the pipeline manipulates and that are stored in csv files via mockdb.

    reports:  This directory contains a number of command line scripts that have been developed to scrape the csv files generated by the mockdb application and return specific fields for reporting purposes.

    processes:  The pipeline and system level (like QsubProcess) objects are defined in this directory.  Also, scripts for dealing with transitions or handeling the pipeline objects are also stored here.  Finally, sub-directories contain the definition objects for specific tasks.

    template:  This is a quick application to find and replace entries in a file demarcated by FIELDBEGIN entry FIELDEND and return a string.  This is mainly used to fill in template qsub files that will be submitted via qsub to SGE.

    utils:  A collection of scripts to allow manipulation of the objects and database independent of running the full data management pipeline script.
