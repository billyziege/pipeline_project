OVERVIEW
  The data_management_pipeline.py script mainly does 2 things:
    1.  Launch jobs for pre-defined pipelines.
    2.  Records parameters for all lauched jobs.

ACTIVATING THE ENVIRONMENT
  To run this pipeline, the data management python environment needs to be activated to ensure dependencies are present.  This is done with the following command on the IHG cluster:
    source /home/sequencing/.virtual_envs/data_management_env/bin/activate
  Since this environment uses python 2.5, the correct python library needs to be connected:
    export LD_LIBRARY_PATH=/home/sequecing/local/lib
  Finally, to ensure that python can find the local scripts, the python path needs to be set:
    export PYTHONPATH=/home/sequencing/src/pipeline_project/pipeline:$PYTHONPATH

LOCAL MODULES/SUB-DIRECTORIES
  Jobs are instantiated via objects which I will discuss more below, and the mockdb application stores these objects in csv files spelled out in the system configuarion file.  To facilitate these functions, a number of modules have been developed:

    class_pedigree:  This application details the inheritance of objects, and returns lists of either ancestor classes (classes from which an object has inheritted behavior) or children classes (classes for which the object is an ancestor).  These lists are used heavily in the mockdb application.

    config:  This heavily uses Python's native ConfigParser.  The system and pipeline specific configuration files are stored in the directory (for instance, system: config/ihg_system.cfg and pipeline: config/std_on_ihg.cfg) are all stored as files ending with the extension cfg.  config/scripts.py contains wrappers to handle specific config entries and return them in the way they are accessed in the pipeline.

    demultiplex_stats:  This application is used after running casava to extract statistics from this process and store it in relevant physical and process objects.

    manage_storage:  This application defines the StorageDevice object which keeps track of specific directories, available storage, and used storage in the manage_storage/models.py.  Queries to the disk (like disk available and disk usage) are handled by manage_storage/disk_queries.py, and more complicated accounting of disk usage for a specified pipeline are handled in managae_storage/scripts.py

    mockdb:

    sge_email:  This application hard-codes the parameters for sending emails on the IHG cluster in the sge_email/models.py file and calls the python packages needed to send an email in sge_email/scripts.py file.  This application may be called from the command line (python sge_email.py -h to obtain information on how to send an email).

    sge_queries:

    physical_objects:  This collection of object definitions defines the attributes of truly physical objects like Sample, Flowcell, and HiSeq machine that the pipeline manipulates and that are stored in csv files via mockdb.

    reports:  This directory contains a number of command line scripts that have been developed to scrape the csv files generated by the mockdb application and return specific fields for reporting purposes.

    processes:

    template:  This is a quick application to find and replace entries in a file demarcated by FIELDBEGIN entry FIELDEND and return a string.  This is mainly used to fill in template qsub files that will be submitted via qsub to SGE.

    utils:  A collection of scripts to allow manipulation of the objects and database independent of running the full data management pipeline script.
